{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28cc302b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/satwiksahoo/Desktop/CodeBasics/machine learning/krish naik/NLP project/ATSresume/RESUME_ATS'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7104f9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c03f2483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataclasses import dataclass\n",
    "# from pathlib import Path\n",
    "\n",
    "# @dataclass\n",
    "# class ModelTrainerConfig:\n",
    "#     root_dir : Path\n",
    "#     transformed_train_path : Path\n",
    "#     transformed_test_path : Path\n",
    "#     trained_model_artifact_path : Path\n",
    "#     output_dir: Path\n",
    "#     eval_strategy : str         # evaluate each epoch\n",
    "#     save_strategy : str      # save best model each epoch\n",
    "#     learning_rate: float        # slightly higher than 2e-5 (good for smaller datasets)\n",
    "#     per_device_train_batch_size: int # larger batch = stabler gradient updates\n",
    "#     per_device_eval_batch_size: int\n",
    "#     num_train_epochs: int            # train a bit longer, but use early stopping\n",
    "#     weight_decay: float              # keep small L2 regularization\n",
    "#     # logging_dir= \"./logs\"\n",
    "#     logging_steps: int\n",
    "#     save_total_limit: int             # keep only last 2 checkpoints\n",
    "#     # load_best_model_at_end:True    # reload best model based on metric\n",
    "#     metric_for_best_model: str     # pick best model using F1 (good for imbalance)\n",
    "#     # greater_is_better:True\n",
    "#     warmup_ratio: float               # gradual warmup → smoother start\n",
    "#     lr_scheduler_type: str     # cosine schedule → better convergence\n",
    "#     gradient_accumulation_steps: int  # effective batch size = 16*2 = 32\n",
    "#     # fp16: True                      # if GPU supports mixed precision (saves memory)\n",
    "#     seed: int                         # reproducibility\n",
    "\n",
    "\n",
    "# config_entity.py\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: str\n",
    "    transformed_train_path: str\n",
    "    transformed_test_path: str\n",
    "    trained_model_artifact_path: str\n",
    "    output_dir: str\n",
    "    eval_strategy: str\n",
    "    save_strategy: str\n",
    "    learning_rate: float\n",
    "    per_device_train_batch_size: int\n",
    "    per_device_eval_batch_size: int\n",
    "    num_train_epochs: int\n",
    "    weight_decay: float\n",
    "    logging_dir: str\n",
    "    logging_steps: int\n",
    "    save_total_limit: int\n",
    "    load_best_model_at_end: bool\n",
    "    metric_for_best_model: str\n",
    "    greater_is_better: bool\n",
    "    warmup_ratio: float\n",
    "    lr_scheduler_type: str\n",
    "    gradient_accumulation_steps: int\n",
    "    fp16: bool\n",
    "    seed: int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cf75247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ATS_RESUME.constants import *\n",
    "from src.ATS_RESUME.utils.common import read_yaml , create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3cb7765",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self, config_file_path=CONFIG_FILE_PATH, params_file_path=PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_file_path)\n",
    "        self.params = read_yaml(params_file_path)\n",
    "        create_directories([self.config.artifact_root])\n",
    "    \n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.TrainingArguments\n",
    "        \n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            transformed_train_path=config.transformed_train_path,\n",
    "            transformed_test_path=config.transformed_test_path,\n",
    "            trained_model_artifact_path=config.trained_model_artifact_path,  # FIXED\n",
    "            output_dir=params.output_dir,\n",
    "            eval_strategy=params.eval_strategy,\n",
    "            save_strategy=params.save_strategy,\n",
    "            learning_rate=params.learning_rate,\n",
    "            per_device_train_batch_size=params.per_device_train_batch_size,\n",
    "            per_device_eval_batch_size=params.per_device_eval_batch_size,\n",
    "            num_train_epochs=params.num_train_epochs,\n",
    "            weight_decay=params.weight_decay,\n",
    "            logging_dir=\"./logs\",\n",
    "            logging_steps=params.logging_steps,\n",
    "            save_total_limit=params.save_total_limit,\n",
    "            load_best_model_at_end=params.load_best_model_at_end,\n",
    "            metric_for_best_model=params.metric_for_best_model,\n",
    "            greater_is_better=params.greater_is_better,\n",
    "            warmup_ratio=params.warmup_ratio,\n",
    "            lr_scheduler_type=params.lr_scheduler_type,\n",
    "            gradient_accumulation_steps=params.gradient_accumulation_steps,\n",
    "            fp16=params.fp16,\n",
    "            seed=params.seed\n",
    "        )\n",
    "        return model_trainer_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1701fe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_from_disk\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, models, evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainerConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def initiate_model_trainer_(self):\n",
    "        # Load tokenized train/test data\n",
    "        train_dataset = load_from_disk(self.config.transformed_train_path)\n",
    "        test_dataset = load_from_disk(self.config.transformed_test_path)\n",
    "\n",
    "        # Load tokenizer + model\n",
    "        model_name = \"distilbert-base-uncased\"   # <--- change as needed\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "        # Training args\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.config.output_dir,\n",
    "            eval_strategy=self.config.eval_strategy,\n",
    "            save_strategy=self.config.save_strategy,\n",
    "            learning_rate=self.config.learning_rate,\n",
    "            per_device_train_batch_size=self.config.per_device_train_batch_size,\n",
    "            per_device_eval_batch_size=self.config.per_device_eval_batch_size,\n",
    "            num_train_epochs=self.config.num_train_epochs,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "            logging_dir=self.config.logging_dir,\n",
    "            logging_steps=self.config.logging_steps,\n",
    "            save_total_limit=self.config.save_total_limit,\n",
    "            load_best_model_at_end=self.config.load_best_model_at_end,\n",
    "            metric_for_best_model=self.config.metric_for_best_model,\n",
    "            greater_is_better=self.config.greater_is_better,\n",
    "            warmup_ratio=self.config.warmup_ratio,\n",
    "            lr_scheduler_type=self.config.lr_scheduler_type,\n",
    "            gradient_accumulation_steps=self.config.gradient_accumulation_steps,\n",
    "            fp16=self.config.fp16,\n",
    "            seed=self.config.seed\n",
    "        )\n",
    "\n",
    "        # Trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=test_dataset,\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        model.save_pretrained(self.config.trained_model_artifact_path)\n",
    "        tokenizer.save_pretrained(self.config.trained_model_artifact_path)\n",
    "    \n",
    "    def initiate_model_trainer(self):\n",
    "        train_file = os.path.join(self.config.transformed_train_path, \"train.pkl\")\n",
    "        test_file = os.path.join(self.config.transformed_test_path, \"test.pkl\")\n",
    "        \n",
    "        with open(train_file, \"rb\") as f:\n",
    "          train_data = pickle.load(f)\n",
    "\n",
    "        with open(test_file, \"rb\") as f:\n",
    "           val_data = pickle.load(f)\n",
    "\n",
    "        \n",
    "        \n",
    "        train_dataloader = DataLoader(train_data, shuffle=True, batch_size=16)\n",
    "        test_dataloader = DataLoader(val_data, shuffle=False, batch_size=16)\n",
    "\n",
    "# Load pre-trained bi-encoder\n",
    "        model = SentenceTransformer(\"sentence-transformers/multi-qa-mpnet-base-dot-v1\")\n",
    "\n",
    "# Loss: Cosine similarity loss (regression on similarity scores)\n",
    "        train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "# Evaluator: compute correlation between predicted similarity & true labels\n",
    "        evaluator = evaluation.EmbeddingSimilarityEvaluator.from_input_examples(val_data, name=\"val\")\n",
    "\n",
    "# Train\n",
    "        model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    evaluator=evaluator,\n",
    "    epochs=6,\n",
    "    warmup_steps=100,\n",
    "    evaluation_steps=500,\n",
    "    output_path=\"./bi_encoder_ats_model\"\n",
    ")\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a37acdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-30 11:38:34,722: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2025-08-30 11:38:34,727: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-08-30 11:38:34,727: INFO: common: created directory at: artifacts]\n",
      "[2025-08-30 11:38:34,729: INFO: common: created directory at: artifacts/model_trainer]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ModelTrainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m ConfigurationManager()\n\u001b[1;32m      2\u001b[0m model_trainer_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_model_trainer_config()\n\u001b[0;32m----> 3\u001b[0m model_trainer \u001b[38;5;241m=\u001b[39m \u001b[43mModelTrainer\u001b[49m(config \u001b[38;5;241m=\u001b[39m model_trainer_config)\n\u001b[1;32m      4\u001b[0m model_trainer\u001b[38;5;241m.\u001b[39minitiate_model_trainer()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ModelTrainer' is not defined"
     ]
    }
   ],
   "source": [
    "config = ConfigurationManager()\n",
    "model_trainer_config = config.get_model_trainer_config()\n",
    "model_trainer = ModelTrainer(config = model_trainer_config)\n",
    "model_trainer.initiate_model_trainer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "34e155ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer , pipeline\n",
    "# from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"artifacts/model_trainer/trained_model_artifact\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"artifacts/model_trainer/trained_model_artifact\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138fffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# def preprocess_text(text):\n",
    "#   tokens = []\n",
    "#   text = re.sub(r'\\r\\n|\\n', ' ', text)\n",
    "#   text = re.sub(r'\\s+', ' ', text)\n",
    "#   text = re.sub(r'<.*?>', ' ', text)\n",
    "#   text = text.strip().lower()\n",
    "\n",
    "\n",
    "\n",
    "#   return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a49ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.9997040629386902}]\n",
      "No Fit\n"
     ]
    }
   ],
   "source": [
    "# classifier = pipeline('text-classification' , model = model , tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "# resume_text = \"\"\"\n",
    "# Data Scientist with 6 years of experience in Python, SQL, and machine learning.\n",
    "# Worked on predictive modeling, NLP, and recommendation systems. Skilled in TensorFlow,\n",
    "# PyTorch, and cloud deployment.\n",
    "# \"\"\"\n",
    "\n",
    "# job_text = \"\"\"\n",
    "# Looking for a Senior Data Scientist with 5+ years of experience in predictive analytics,\n",
    "# machine learning, and deep learning. Strong Python and SQL skills required. \n",
    "# Experience with cloud ML deployment is a plus.\n",
    "# \"\"\"\n",
    "\n",
    "# # Concatenate resume and job description as input\n",
    "# test_input = preprocess_text(resume_text) + \" [SEP] \" + preprocess_text(job_text) # /// q\n",
    "\n",
    "# prediction = classifier(test_input)\n",
    "# print(prediction)\n",
    "\n",
    "# mapping = {  'LABEL_0':'No Fit'  ,  'LABEL_1' : 'Potential Fit' ,  'LABEL_2' : 'Good Fit'   }\n",
    "\n",
    "# print(mapping[prediction[0]['label']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a258bca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'LABEL_0', 1: 'LABEL_1', 2: 'LABEL_2'}\n"
     ]
    }
   ],
   "source": [
    "# print(model.config.id2label)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a1901d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# resume_text = \"\"\"\n",
    "# Data Scientist with 6 years of experience in Python, SQL, and machine learning.\n",
    "# Worked on predictive modeling, NLP, and recommendation systems. Skilled in TensorFlow,\n",
    "# PyTorch, and cloud deployment.\n",
    "# \"\"\"\n",
    "\n",
    "# job_text = \"\"\"\n",
    "# Looking for a Senior Data Scientist with 5+ years of experience in predictive analytics,\n",
    "# machine learning, and deep learning. Strong Python and SQL skills required. \n",
    "# Experience with cloud ML deployment is a plus.\n",
    "# \"\"\"\n",
    "\n",
    "# prediction = classifier((resume_text, job_text))   # let tokenizer handle [SEP]\n",
    "# print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f61d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: No Fit\n",
      "Probabilities: [[0.9997040629386902, 0.00024037726689130068, 5.5546257499372587e-05]]\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# # # Load tokenizer and model\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(\"path/to/your/model\")\n",
    "# # model = AutoModelForSequenceClassification.from_pretrained(\"path/to/your/model\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"artifacts/model_trainer/trained_model_artifact\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"artifacts/model_trainer/trained_model_artifact\")\n",
    "# resume_text = \"\"\"\n",
    "# Data Scientist with 6 years of experience in Python, SQL, and machine learning.\n",
    "# Worked on predictive modeling, NLP, and recommendation systems. Skilled in TensorFlow,\n",
    "# PyTorch, and cloud deployment.\n",
    "# \"\"\"\n",
    "\n",
    "# job_text = \"\"\"\n",
    "# Looking for a Senior Data Scientist with 5+ years of experience in predictive analytics,\n",
    "# machine learning, and deep learning. Strong Python and SQL skills required. \n",
    "# Experience with cloud ML deployment is a plus.\n",
    "# \"\"\"\n",
    "\n",
    "# # Concatenate resume and job description\n",
    "# test_input = preprocess_text(resume_text) + \" [SEP] \" + preprocess_text(job_text)\n",
    "\n",
    "# # Tokenize\n",
    "# inputs = tokenizer(test_input, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# # Forward pass\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**inputs)\n",
    "\n",
    "# # Get logits\n",
    "# logits = outputs.logits\n",
    "\n",
    "# # Apply softmax to get probabilities\n",
    "# probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "# # Get predicted label index\n",
    "# predicted_class_id = torch.argmax(probs, dim=-1).item()\n",
    "\n",
    "# # Map id -> label\n",
    "# id2label = model.config.id2label  # e.g., {0: \"LABEL_0\", 1: \"LABEL_1\", 2: \"LABEL_2\"}\n",
    "\n",
    "# # Custom mapping\n",
    "# mapping = {\n",
    "#     \"LABEL_0\": \"No Fit\",\n",
    "#     \"LABEL_1\": \"Potential Fit\",\n",
    "#     \"LABEL_2\": \"Good Fit\"\n",
    "# }\n",
    "\n",
    "# print(\"Prediction:\", mapping[id2label[predicted_class_id]])\n",
    "# print(\"Probabilities:\", probs.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48636a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-30 11:42:40,481: INFO: SentenceTransformer: Use pytorch device_name: mps]\n",
      "[2025-08-30 11:42:40,482: INFO: SentenceTransformer: Load pretrained SentenceTransformer: artifacts/model_trainer/model5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 49.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 58.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 0.7148\n",
      "Predicted Label: Good Fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses, models, evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer(\"artifacts/model_trainer/model5\")\n",
    "\n",
    "\n",
    "resume = \"\"\"\n",
    "Marketing Specialist with 7 years of experience in SEO, content marketing,\n",
    "and social media strategy. Recently completed a data analytics certification \n",
    "with hands-on projects in Python and SQL.\n",
    "\"\"\"\n",
    "\n",
    "jd = \"\"\"\n",
    "Looking for a Senior Data Scientist with 5+ years of experience in predictive analytics,\n",
    "machine learning, and deep learning. Strong Python and SQL skills required. \n",
    "Experience with cloud ML deployment is a plus.\n",
    "\"\"\"\n",
    "emb_resume = model.encode(resume, convert_to_tensor=True)\n",
    "emb_jd = model.encode(jd, convert_to_tensor=True)\n",
    "\n",
    "similarity = util.cos_sim(emb_resume, emb_jd).item()\n",
    "print(f\"Similarity Score: {similarity:.4f}\")\n",
    "\n",
    "# Thresholds (tune with validation set)\n",
    "if similarity < 0.4:\n",
    "    label = \"No Fit\"\n",
    "elif similarity < 0.7:\n",
    "    label = \"Potential Fit\"\n",
    "else:\n",
    "    label = \"Good Fit\"\n",
    "\n",
    "print(f\"Predicted Label: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f87be7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
